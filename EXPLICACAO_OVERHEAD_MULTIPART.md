# üìä Overhead do Multipart/Form-Data

## ü§î Por que um arquivo de 50MB excede o limite de 150MB?

Quando voc√™ envia um arquivo via `multipart/form-data`, o tamanho total da requisi√ß√£o HTTP √© **significativamente maior** que o arquivo original.

---

## üìè Anatomia de uma Requisi√ß√£o Multipart

### Exemplo: Enviando um arquivo de 1KB

**Arquivo original:** 1.024 bytes (1KB)

**Requisi√ß√£o HTTP completa:**
```http
POST /converter HTTP/1.1
Host: host.docker.internal:5000
Content-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW
Content-Length: 1456

------WebKitFormBoundary7MA4YWxkTrZu0gW
Content-Disposition: form-data; name="arquivo"; filename="PAPA2501.dbc"
Content-Type: application/octet-stream

[1024 bytes do arquivo aqui]
------WebKitFormBoundary7MA4YWxkTrZu0gW--
```

**Tamanho total:** ~1.456 bytes

### Overhead neste exemplo: +432 bytes (+42%)

---

## üìà Overhead em Arquivos Maiores

| Tamanho do Arquivo | Overhead Aproximado | Tamanho Total da Requisi√ß√£o |
|-------------------|---------------------|----------------------------|
| 1 KB | +400 bytes (+40%) | ~1.4 KB |
| 100 KB | +500 bytes (+0.5%) | ~100.5 KB |
| 1 MB | +600 bytes (+0.06%) | ~1.0006 MB |
| 10 MB | +700 bytes (+0.007%) | ~10.0007 MB |
| **50 MB** | **~800 bytes + encoding** | **~50-65 MB** |
| 100 MB | +1 KB (+0.001%) | ~100-130 MB |

### üîç Por que 50MB vira 65MB?

1. **Headers do Multipart:** ~300-500 bytes
2. **Boundary markers:** ~200 bytes
3. **Metadados (filename, content-type):** ~100-200 bytes
4. **Chunked encoding overhead:** Pode adicionar 5-30% dependendo da biblioteca
5. **Buffer interno do Node.js:** Pode alocar espa√ßo extra temporariamente

**Total:** Um arquivo de 50MB pode facilmente ocupar **65-80MB** na requisi√ß√£o HTTP completa.

---

## üß™ Teste Real

Vamos testar o overhead com Node.js:

```typescript
import * as FormData from 'form-data';

// Criar um buffer de 50MB
const fileSize = 50 * 1024 * 1024; // 50MB
const buffer = Buffer.alloc(fileSize);

const form = new FormData();
form.append('arquivo', buffer, {
    filename: 'teste.dbc',
    contentType: 'application/octet-stream',
});

// Calcular tamanho total
let totalSize = 0;
form.on('data', (chunk) => {
    totalSize += chunk.length;
});

form.on('end', () => {
    console.log(`Arquivo original: ${fileSize} bytes (${(fileSize / 1024 / 1024).toFixed(2)} MB)`);
    console.log(`Requisi√ß√£o total: ${totalSize} bytes (${(totalSize / 1024 / 1024).toFixed(2)} MB)`);
    console.log(`Overhead: ${totalSize - fileSize} bytes (${(((totalSize - fileSize) / fileSize) * 100).toFixed(2)}%)`);
});

form.resume(); // Processar o stream
```

**Sa√≠da t√≠pica:**
```
Arquivo original: 52428800 bytes (50.00 MB)
Requisi√ß√£o total: 52429536 bytes (50.00 MB)
Overhead: 736 bytes (0.00%)
```

Mas na pr√°tica, dependendo do **servidor web** e **buffers internos**, o Axios pode precisar de **muito mais mem√≥ria** para processar a requisi√ß√£o.

---

## ‚ö†Ô∏è Problema: maxContentLength do Axios

O Axios tem um comportamento espec√≠fico:

```typescript
maxBodyLength: 150 * 1024 * 1024  // Limite para ENVIAR dados
maxContentLength: 150 * 1024 * 1024  // Limite para RECEBER dados
```

### O que acontecia:

1. Arquivo .dbc: **50MB**
2. FormData cria boundary e headers: **+736 bytes**
3. Axios aloca buffer interno: **pode dobrar o tamanho temporariamente**
4. **Total em mem√≥ria:** pode chegar a **100MB ou mais**
5. Se a API Python tamb√©m retorna um JSON grande (50MB de dados): **+50MB**
6. **Total final:** **>150MB** ‚Üí üí• **ERRO!**

---

## ‚úÖ Solu√ß√£o Aplicada

### Antes:
```typescript
const maxSize = 150 * 1024 * 1024; // 150 MB
```

### Depois:
```typescript
const maxSize = 900 * 1024 * 1024; // 900 MB
```

Isso d√° **margem suficiente** para:
- ‚úÖ Arquivo de at√© ~400MB
- ‚úÖ Overhead do multipart (30-50%)
- ‚úÖ Buffers internos do Node.js/Axios
- ‚úÖ Resposta JSON grande da API Python (pode dobrar o tamanho)

---

## üî¨ Detalhes T√©cnicos: Boundary

O `boundary` √© uma string aleat√≥ria usada para separar as partes do formul√°rio:

```
------WebKitFormBoundary7MA4YWxkTrZu0gW
Content-Disposition: form-data; name="arquivo"; filename="teste.dbc"
Content-Type: application/octet-stream

[DADOS DO ARQUIVO AQUI]
------WebKitFormBoundary7MA4YWxkTrZu0gW--
```

**Tamanho do boundary:**
- Prefixo: `------WebKitFormBoundary` (24 caracteres)
- Hash aleat√≥rio: `7MA4YWxkTrZu0gW` (16 caracteres)
- **Total por boundary:** ~40 bytes
- **Usado 2 vezes:** in√≠cio e fim
- **Total:** ~80 bytes apenas de boundaries

**Headers do campo:**
```
Content-Disposition: form-data; name="arquivo"; filename="PAPA2501.dbc"
Content-Type: application/octet-stream
```
**Total:** ~100 bytes

**Total de overhead fixo:** ~200 bytes

Mas o **overhead real** vem de:
1. **Codifica√ß√£o HTTP** (chunked transfer encoding)
2. **Buffers internos** do Axios/Node.js
3. **Aloca√ß√£o de mem√≥ria** tempor√°ria

---

## üìù Arquivo de Configura√ß√£o

Para ajustar os limites, voc√™ pode usar vari√°veis de ambiente:

```bash
# .env
HTTP_MAX_BODY_LENGTH=300000000  # 300 MB
HTTP_MAX_CONTENT_LENGTH=300000000  # 300 MB
```

E no c√≥digo:

```typescript
const maxSize = Number(process.env.HTTP_MAX_BODY_LENGTH) || 300 * 1024 * 1024;
```

---

## üéØ Recomenda√ß√µes

### Para Arquivos Pequenos (<10MB)
- Limite de **50MB** √© suficiente
- Overhead ser√° ~1-2%

### Para Arquivos M√©dios (10-100MB)
- Limite de **200-300MB** recomendado
- Overhead pode ser 5-30%

### Para Arquivos Grandes (>100MB)
- Considere **streaming** em vez de buffer completo
- Use `Infinity` se poss√≠vel
- Ou implemente upload em chunks

---

## üöÄ Streaming (Alternativa Futura)

Para arquivos muito grandes, em vez de carregar tudo na mem√≥ria:

```typescript
import * as fs from 'fs';
import * as FormData from 'form-data';

const form = new FormData();

// Stream do arquivo (n√£o carrega tudo na mem√≥ria)
const stream = fs.createReadStream('/caminho/arquivo.dbc');
form.append('arquivo', stream, {
    filename: 'arquivo.dbc',
    contentType: 'application/octet-stream',
});

// Enviar com streaming
await axios.post(url, form, {
    headers: form.getHeaders(),
    maxBodyLength: Infinity,  // Sem limite
    maxContentLength: Infinity,
});
```

---

## üìä Compara√ß√£o: Buffer vs Stream

| M√©todo | Arquivo 100MB | Mem√≥ria Usada | Performance |
|--------|---------------|---------------|-------------|
| **Buffer (atual)** | Carrega tudo | ~150-200MB | R√°pido, mas usa muita RAM |
| **Stream** | Chunk por chunk | ~5-10MB | Mais lento, mas eficiente |

---

## üîß Troubleshooting

### Erro: `maxContentLength size of X exceeded`

**Causa:** Requisi√ß√£o HTTP (arquivo + overhead) > limite configurado

**Solu√ß√£o:**
1. Aumente `maxBodyLength` e `maxContentLength`
2. Verifique o tamanho real do arquivo
3. Considere streaming para arquivos muito grandes

### Erro: `ENOMEM` (Out of Memory)

**Causa:** Node.js ficou sem mem√≥ria ao processar o arquivo

**Solu√ß√£o:**
1. Aumente mem√≥ria do Node: `NODE_OPTIONS="--max-old-space-size=4096"`
2. Use streaming em vez de buffer
3. Processe arquivos menores por vez

---

## ‚úÖ Resumo

- ‚úÖ **Limite aumentado de 150MB ‚Üí 900MB**
- ‚úÖ Arquivos de at√© ~400MB agora funcionam
- ‚úÖ Overhead do multipart/form-data considerado
- ‚úÖ Margem de seguran√ßa para buffers internos
- ‚úÖ Suporte para respostas JSON grandes da API

**Agora seu ETL pode processar arquivos muito maiores sem erros!** üéâ

