# ‚ö° Otimiza√ß√£o da API Python - Processamento DBC

## üêå Problema Identificado

A API Flask est√° demorando **muito tempo** para processar arquivos .dbc:
- **29 segundos** para um arquivo de teste pequeno (fake)
- Arquivos reais (.dbc com v√°rios MB) podem demorar **minutos**
- Isso causa timeout e `ECONNRESET` no cliente

## ‚úÖ Solu√ß√µes Aplicadas no TypeScript

### 1. Timeout aumentado para 30 minutos
- `datasus.service.ts`: timeout de 1.800.000ms (30 minutos)
- `http-client.module.ts`: timeout padr√£o de 30 minutos
- `docker-compose.yml`: HTTP_TIMEOUT=1800000

Isso **resolve o erro imediato**, mas a API continua lenta.

---

## üöÄ Recomenda√ß√µes para Otimizar a API Python

### Problema 1: Servidor de Desenvolvimento (Werkzeug)

O servidor embutido do Flask **n√£o √© otimizado** para produ√ß√£o.

**Solu√ß√£o: Use Gunicorn ou uWSGI**

```bash
# Instalar Gunicorn
pip install gunicorn

# Rodar com Gunicorn
gunicorn --workers 4 --timeout 300 --bind 0.0.0.0:5000 app:app
```

Ou crie um `Dockerfile` para a API Python:

```dockerfile
FROM python:3.12-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app.py .

# Use Gunicorn em vez do servidor de desenvolvimento
CMD ["gunicorn", "--workers", "4", "--timeout", "300", "--bind", "0.0.0.0:5000", "app:app"]
```

---

### Problema 2: Processamento S√≠ncrono

Cada requisi√ß√£o trava o servidor at√© terminar.

**Solu√ß√£o A: Limite de registros**

Sua API Python j√° tem suporte para `limit` via form-data, mas n√£o est√° sendo usado:

```python
# No seu endpoint /converter
limit = request.form.get('limit', type=int, default=None)

# Ao ler o DBF
contador = 0
for registro in table:
    if limit and contador >= limit:
        break
    registro_limpo = {}
    # ... processar registro ...
    dados_json.append(registro_limpo)
    contador += 1
```

**Solu√ß√£o B: Processamento ass√≠ncrono com Celery**

Para arquivos grandes, retorne um job ID e processe em background:

```python
from celery import Celery

celery = Celery('tasks', broker='redis://localhost:6379')

@celery.task
def processar_dbc_async(arquivo_path):
    # Processamento pesado aqui
    return resultado

@app.route('/converter', methods=['POST'])
def converter_dbc():
    # Salva arquivo
    task = processar_dbc_async.delay(temp_path)
    return jsonify({'job_id': task.id, 'status': 'processing'})

@app.route('/status/<job_id>')
def check_status(job_id):
    task = celery.AsyncResult(job_id)
    if task.ready():
        return jsonify({'status': 'done', 'result': task.result})
    return jsonify({'status': 'processing'})
```

---

### Problema 3: Processamento de Arquivos Grandes

Ler todo o arquivo DBF na mem√≥ria pode ser lento.

**Solu√ß√£o: Streaming/Chunking**

```python
@app.route('/converter', methods=['POST'])
def converter_dbc():
    # ... valida√ß√µes ...
    
    # Processar em chunks
    CHUNK_SIZE = 1000
    dados_json = []
    
    for i, registro in enumerate(table):
        # ... processar registro ...
        dados_json.append(registro_limpo)
        
        # Se chegou ao chunk, envie parcialmente
        if (i + 1) % CHUNK_SIZE == 0:
            # Opcional: salvar em DB parcialmente
            pass
    
    return jsonify({
        'sucesso': True,
        'total_registros': len(dados_json),
        'dados': dados_json  # Ou salvar em DB e n√£o retornar todos
    })
```

**Melhor ainda: N√£o retorne todos os dados**

Se voc√™ tem milhares de registros, n√£o faz sentido retornar todos no JSON. Salve direto no banco de dados:

```python
import psycopg2

@app.route('/converter', methods=['POST'])
def converter_dbc():
    # ... processar DBC ...
    
    # Conectar ao banco
    conn = psycopg2.connect(
        host='postgres',
        database='etl_mult',
        user='postgres',
        password='postgres'
    )
    cursor = conn.cursor()
    
    # Inserir em batch
    for registro in table:
        cursor.execute(
            "INSERT INTO datasus_registros (coluna1, coluna2, ...) VALUES (%s, %s, ...)",
            (valor1, valor2, ...)
        )
    
    conn.commit()
    cursor.close()
    conn.close()
    
    # Retornar apenas metadados
    return jsonify({
        'sucesso': True,
        'arquivo': arquivo.filename,
        'total_registros': total_registros,
        'total_colunas': len(colunas),
        'colunas': colunas,
        # N√ÉO retorne 'dados' se tiver milhares de registros
    })
```

---

### Problema 4: Biblioteca `pyreaddbc` pode ser lenta

A fun√ß√£o `dbc2dbf()` pode ser o gargalo.

**Solu√ß√£o: Investigate ou troque a biblioteca**

```python
import time

@app.route('/converter', methods=['POST'])
def converter_dbc():
    # ... c√≥digo anterior ...
    
    start = time.time()
    dbc2dbf(temp_path, temp_dbf_path)
    dbc_time = time.time() - start
    print(f"‚è±Ô∏è dbc2dbf levou {dbc_time:.2f}s")
    
    start = time.time()
    table = dbfread.DBF(temp_dbf_path, encoding='latin1')
    dbf_time = time.time() - start
    print(f"‚è±Ô∏è dbfread levou {dbf_time:.2f}s")
    
    # ... resto do c√≥digo ...
```

Veja qual parte est√° mais lenta e otimize.

---

## üê≥ Docker Compose para a API Python

Adicione a API Python no seu `docker-compose.yml`:

```yaml
services:
  # ... postgres, redis, etl_job ...
  
  converter_api:
    build:
      context: ./converter-api  # Pasta com Dockerfile da API Python
    container_name: converter_api
    restart: unless-stopped
    ports:
      - "5000:5000"
    environment:
      - GUNICORN_WORKERS=4
      - GUNICORN_TIMEOUT=300
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/"]
      interval: 30s
      timeout: 10s
      retries: 3
```

E ent√£o no `etl_job`, use o nome do servi√ßo:

```yaml
etl_job:
  environment:
    CONVERTER_API_URL: http://converter_api:5000/converter
```

---

## üìä Monitoramento

Adicione logs detalhados para identificar gargalos:

```python
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@app.route('/converter', methods=['POST'])
def converter_dbc():
    start_total = time.time()
    
    try:
        logger.info(f"üì• Recebendo arquivo: {arquivo.filename}")
        
        # Salvar arquivo
        start = time.time()
        arquivo.save(temp_path)
        logger.info(f"‚è±Ô∏è Salvar arquivo: {time.time() - start:.2f}s")
        
        # Convers√£o DBC -> DBF
        start = time.time()
        dbc2dbf(temp_path, temp_dbf_path)
        logger.info(f"‚è±Ô∏è dbc2dbf: {time.time() - start:.2f}s")
        
        # Leitura DBF
        start = time.time()
        table = dbfread.DBF(temp_dbf_path, encoding='latin1')
        logger.info(f"‚è±Ô∏è Abrir DBF: {time.time() - start:.2f}s")
        
        # Processamento
        start = time.time()
        dados_json = []
        for registro in table:
            # ... processar ...
            dados_json.append(registro_limpo)
        logger.info(f"‚è±Ô∏è Processar registros: {time.time() - start:.2f}s")
        
        logger.info(f"‚úÖ Total: {time.time() - start_total:.2f}s - {len(dados_json)} registros")
        
        return jsonify({...})
    
    except Exception as e:
        logger.error(f"‚ùå Erro ap√≥s {time.time() - start_total:.2f}s: {str(e)}")
        raise
```

---

## üéØ Resumo das A√ß√µes

### ‚úÖ J√° Feito (TypeScript)
- [x] Timeout aumentado para 30 minutos
- [x] Configura√ß√µes no docker-compose
- [x] Tratamento de erros melhorado

### üîß Fazer na API Python (Recomendado)
1. **Urgente:** Trocar Werkzeug por Gunicorn
2. **Importante:** Adicionar logs de performance
3. **Performance:** Implementar limite de registros
4. **Arquitetura:** N√£o retornar todos os dados no JSON (salvar no DB)
5. **Produ√ß√£o:** Dockerizar a API Python
6. **Avan√ßado:** Processamento ass√≠ncrono com Celery

---

## üß™ Teste Ap√≥s Otimiza√ß√µes

```bash
# Rebuildar
cd /home/luan/Documentos/etl-mult/etl-mult
docker-compose down
docker-compose build
docker-compose up

# Monitorar logs
docker-compose logs -f etl_job
```

Com as otimiza√ß√µes, o processamento deve ficar muito mais r√°pido! üöÄ




